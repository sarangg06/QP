High-level strategy (fast + clean)

You want controlled combinatorics, not free-form generation.

Target

~150â€“250 SQL skeletons

20â€“40 NL variants per skeleton

= 7,000â€“9,000 pairs

Step 1: Extract your schema (1â€“2 hours)

Export:

Table names

Column names

Primary / foreign keys

Business meanings (comments if available)

Create a schema dictionary:

{
  "employees": {
    "columns": ["emp_id", "name", "dept_id", "salary", "status"],
    "meaning": "All full-time employees"
  },
  "departments": {
    "columns": ["dept_id", "dept_name"],
    "meaning": "Organizational departments"
  }
}


This becomes ground truth.

Step 2: Design SQL skeletons (MOST IMPORTANT) (1â€“2 days)

These are parameterized SQL templates.

Example skeleton
SELECT d.dept_name, AVG(e.salary) AS avg_salary
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
WHERE e.status = '{STATUS}'
GROUP BY d.dept_name;


Variables:

{STATUS} â†’ active, inactive, terminated

How many skeletons do you need?
SQL Type	Skeletons
Simple SELECT	30â€“40
JOINs	60â€“80
GROUP BY	40â€“60
Subqueries / CTE	20â€“30
Edge cases	10â€“20

ğŸ¯ Total: ~180â€“220 skeletons

Step 3: Auto-expand skeletons into SQL (SCRIPTED) (Â½ day)

Use Python â€” not an LLM â€” for this.

from itertools import product

statuses = ["active", "inactive", "terminated"]
years = [2022, 2023, 2024]

queries = []

for s, y in product(statuses, years):
    queries.append(
        f"""
        SELECT d.dept_name, AVG(e.salary)
        FROM employees e
        JOIN departments d ON e.dept_id = d.dept_id
        WHERE e.status = '{s}' AND e.year = {y}
        GROUP BY d.dept_name;
        """
    )


This gives you:

Correct SQL

Guaranteed coverage

Zero hallucination

Step 4: Generate NL variants using an LLM (FAST) (Â½â€“1 day)

Now let the LLM do what itâ€™s good at: language variation.

Prompt example:

Given this SQL query:

<SQL>

Generate 10 DISTINCT natural language questions
that a business user would ask.

Rules:
- Do NOT mention SQL
- Use organizational language
- Vary phrasing, tone, and abstraction


Each SQL â†’ 10â€“20 NL queries.

Output structure
{
  "question": "Which departments have the highest average pay?",
  "sql": "SELECT d.dept_name..."
}

Step 5: Deduplicate aggressively (AUTOMATED) (Â½ day)

This step matters more than you think.

Use embeddings similarity:

SentenceTransformers

Cosine similarity threshold: 0.90â€“0.95

Remove near-duplicates:

â€œhighest paidâ€

â€œtop salaryâ€

â€œmaximum compensationâ€

Keep semantic diversity.

Step 6: Human review (targeted, not exhaustive) (1â€“2 days)

Do NOT review everything.

Instead:

Random sample 10%

Mandatory review:

Long queries

Subqueries

Joins >3 tables

Business-critical tables

Typical yield:

2â€“5% fixes

Huge quality improvement

Step 7: Inject â€œhardâ€ examples (manual) (Â½ day)

Add ~300â€“500 intentionally tricky queries:

Ambiguous wording

Implicit joins

Business shorthand

Example:

"Whoâ€™s costing us the most this quarter?"


These teach the model organizational semantics.

Time & Effort Summary
Step	Time
Schema prep	1â€“2 hrs
SQL skeletons	1â€“2 days
Auto expansion	Â½ day
NL generation	Â½â€“1 day
Deduplication	Â½ day
Review	1â€“2 days

ğŸ•’ Total: ~5â€“7 working days

Why this is the MOST efficient way

âœ… No hallucinated SQL
âœ… Humans focus only where it matters
âœ… Perfect label alignment
âœ… Scales linearly
âœ… Audit-friendly

Most teams that â€œjust prompt GPTâ€ end up throwing away 40â€“60% of the data.

One advanced trick (optional but powerful)

Train in curriculum order:

Simple SELECT

JOIN

GROUP BY

Subqueries

It stabilizes LoRA training and converges faster.

Final recommendation

If you want 7â€“9k queries fast and clean:

Design ~200 SQL skeletons â†’ expand â†’ LLM for language â†’ dedupe â†’ review

Thatâ€™s the highest ROI path I know.
